# SALMON Testing Approach

The SALMON project employs the following types of tests:

- **Unit Tests**: Lightweight, module-level tests that ensure individual components function as expected.
- **Integration Tests**: These tests validate how AWS services utilized by SALMON interact with each other and perform the intended functions.
- **Deployment Tests**: These tests verify the correctness of the CDK (Cloud Development Kit) deployment.

Tests are automatically executed when merging into the `main` branch and on various other occasions. Running these tests on pull request merges ensures that the application functions correctly and that no breaking changes are introduced.

If you plan to contribute to the SALMON project, you mayb choose to run the tests mentioned above before creating a pull request. This article provides an overview of how to configure, prepare, and execute these tests, whether through GitHub Actions workflows or locally.

## Unit Tests

### Configuration and Prerequisites

1. Create a Python virtual environment (you can also reuse the one used for deployment).
2. Install the necessary dependencies from `requirements-test.txt`.

### Running Unit Tests

To run the unit tests, execute the following command:

```bash
pytest tests/
```

## Integration tests

### How Integration tests are designed

In order to execute integration tests, the separate isolated environment is spinned-up, which includes:
   1. Standard SALMON resources (`tooling` and `monitored` environments).
   2. Resources specifically created to conduct tests:
      1. `Testing stand resources` (e.g. glue_jobs) of each resource_type handled by SALMON. Typically one resource for type which always fails (to test alerts/failures etc.) and one which always succeeds.
      2. `Infrastructure to catch alerts and other messages` (e.g. digest) generated by SALMON upon events happening in testing stand resources. Namely, an SNS topic as a destination for all messages and DynamoDB table to persistently store them.
   3. `JSON configuration files` to orchestrate interaction between SALMON and testing resources. Specifically, SNS topic referred in `recipients.json` as a main destination and monitoring groups are defined to refer testing stand resources.

Default name for the environment is `devit`.

![Integration Tests Environment](/docs/images/inttests-infra.png "Integration Tests Environment")

Tests execution includes the following steps:

1. All AWS Resources are created (including SALMON and integration test specific resources).
2. Testing stand execution:
   1. All relevant testing stand resources are triggered (also waiting for their completion). This phases generates required alerts (upon designated resource execution failures).
   2. Extract-metrics-orch lambda is executed (this step stores metrics in Timestream table).
   3. Digest lambda is executed (to generate execution digest for all resources).
3. Test results analysis (done in form of pytest test). It includes the following categories:
   1. Common for all resource (such as, "we didn't observe any internal SALMON failures", "digest message is generated succesfully").
   2. Specific for each resource type (including checking metrics values, expected alert messages arrivals etc.)
4. All AWS Resources created in step #1 are deleted.

![Integration Tests Workflow](/docs/images/inttests-steps.png "Integration Tests Workflow")

### Configuration and prerequisistes

1. `IAM service user for github actions` is required. You can create the user with cdk application `github_actions_resources`:
- Browse to folder `cdk/github_actions_resources`
- Run command `cdk deploy` (this will create a stack with IAM user, its permissions and a secret in Secrets manager containing AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY for the user).
- In your github repository create repository secrets (Settings -> Secrets and Variables -> Action). Add two secrets named AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY containing respective values from Secrets Manager.

2. `SALMON configuration files` for integration tests environment are stored in `integration_tests/settings` folder.  
You can customize these files (pushing changes to repository is needed).

### Running integration tests via Github Workflows

If you want to run full integration tests workflow in GitHub, no further preparations are needed, just trigger `Integration Tests - Full Workflow` action.

In case you intend to repeat test execution process multiple times:
1. Run workflow `Integration Tests - Create Infra` workflow. This will create all required AWS resources.
2. Now you can execute integration tests triggering workflow `Integration Tests - Execute Tests` as many times as needed.
3. When you are done - run `Integration Tests - Delete Infra` workflow to tear down resources.

### Running integration tests on a local machine

Running test execution process locally gives you additional flexibility as you can decouple processes of testing stand execution and test results analysis (which can be useful, for example, when you are debugging pytest portion of the code).

1. Run workflow `Integration Tests - Create Infra` workflow. This will create all required AWS resources. Please note the stage-name used for resource creation (by default, it's `devit`).
2. Now you can run scripts locally:

The following command triggers testing stand execution (replace parameter values, if needed):
```bash
  python integration_tests/testing_stand_execution.py --stage-name devit --region eu-central-1
```
First output line of the script gives you a start-time (in a format of epoch milliseconds). Later, you will use this parameter for running pytest (to filter out artifacts produced specifically during current testing stand execution).

Running test results analysis (which is done in a form of pytest tests):
```bash
  pytest integration_tests/tests --stage-name devit --region eu-central-1 --start-epochtimemsec 123456789000
```   
3. When you are done - run `Integration Tests - Delete Infra` workflow to tear down resources.


## Deployment tests

### How Deployment tests are designed

### Configuration and prerequisistes

### Running deployment tests