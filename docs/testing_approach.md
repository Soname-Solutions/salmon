# SALMON Testing Approach

The SALMON project employs the following types of tests:

- **Unit Tests**: Lightweight, module-level tests that ensure individual components function as expected.
- **Integration Tests**: These tests validate how AWS services utilized by SALMON interact with each other and perform the intended functions.
- **Deployment Tests**: These tests verify the correctness of the CDK (Cloud Development Kit) deployment.

Tests are automatically executed when merging into the `main` branch and on various other occasions. Running these tests on pull request merges ensures that the application functions correctly and that no breaking changes are introduced.

If you plan to contribute to the SALMON project, you mayb choose to run the tests mentioned above before creating a pull request. This article provides an overview of how to configure, prepare, and execute these tests, whether through GitHub Actions workflows or locally.

## Unit Tests

### Configuration and Prerequisites

1. Create a Python virtual environment (you can also reuse the one used for deployment).
2. Install the necessary dependencies from `requirements-test.txt`.

### Running Unit Tests

To run the unit tests, execute the following command:

```bash
pytest tests/
```

## Integration tests

### How Integration tests are designed

In order to execute integration test, the separate isolated environment is spinned-up, which includes:
   1. Standard SALMON resources (`tooling` and `monitored` environments).
   2. Resources specifically created to conduct tests:
      1. `Testing stand resources` (e.g. glue_jobs) of each resource_type handled by SALMON. Typically one resource for type which always fails (to test alerts/failures etc.) and one which always succeeds.
      2. `Infrastructure to catch alerts and other messages` (e.g. digest) generated by SALMON upon events happening in testing stand resources. Namely, an SNS topic as a destination for all messages and DynamoDB table to persistently store them.
   3. `JSON configuration files` to orchestrate interaction between SALMON and testing resources. Specifically, SNS topic referred in recipient.json as a main destination and monitoring groups are defined to refer testing stand resources.

![Integration Tests Environment](/docs/images/inttests-infra.png "Integration Tests Environment")

Tests execution includes the following steps:

1. All AWS Resources are created (including SALMON and integration test specific resources).
2. Testing stand execution:
   1. All relevant testing stand resources are triggers (also waiting for theit completion). This phases generates required alerts (upon designated resource execution failures).
   2. Extract-metrics-orch lambda is executed (this stores metrics in Timestream table).
   3. Digest lambda is executed (to generate execution digest for all resources).
3. Test results analysis (done in form of pytest test). It includes the following categories:
   1. Common for all resource (such as, "we didn't observe any internal SALMON failures", "digest message is generated succesfully").
   2. Specific for each resource type (including checking metrics values, expected alert messages arrivals etc.)

![Integration Tests Workflow](/docs/images/inttests-steps.png "Integration Tests Workflow")

### Configuration and prerequisistes

If you want to run full integration tests workflow in GitHub, no preparation is needed, just trigger "Integration Tests - Full Workflow" action.

In case you intend to execute tests locally or repeat process multiple team.

### Running integration tests via Github Workflows

### Running integration tests on local machine

## Deployment tests

### How Deployment tests are designed

### Configuration and prerequisistes

### Running deployment tests